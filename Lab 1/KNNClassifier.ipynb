{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We first import the necessary packages"
      ],
      "metadata": {
        "id": "Ornkjp7YPwdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "y6U3GHRtEiSV"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create labels for the attributes before reading in the data."
      ],
      "metadata": {
        "id": "LiIi9cJUabRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset and transform to pandas dataframe\n",
        "\n",
        "columns = [\"ID\",\"Diagnosis\",\"radius\",\"texture\",\"perimeter\",\"area\",\"smoothness\",\"compactness\",\"concavity\",\"concave points\",\"symmetry\",\"fractal dimension\"]\n",
        "\n",
        "def create_labels(columns):\n",
        "  dataset_labels = [\"ID\",\"Diagnosis\"]\n",
        "  # iterate over columns and\n",
        "  for attribute in columns[2:len(columns)]:\n",
        "    dataset_labels.extend([f\"SE_{attribute}\",attribute,f\"Worst_{attribute}\"])\n",
        "\n",
        "  return dataset_labels\n",
        "\n",
        "# create dataset labels\n",
        "dataset_labels = create_labels(columns)"
      ],
      "metadata": {
        "id": "Tv9uGCUEFWy1"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then read the data file.\n",
        "\n",
        "Enter the File path in the file_path variable **below**\n",
        "\n",
        "Ensure that the datafile has been uploaded to colab\n",
        "\n",
        "Ensure that the file path is a **string**"
      ],
      "metadata": {
        "id": "EIbvOgZSnsaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/wdbc.data\"\n",
        "\n",
        "df = pd.read_csv(file_path,names = dataset_labels, header = None)\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "BXZV6pmQaVvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove duplicate data from the dataset and fill missing data by replacing it with the class(Diagnosis) mean for each attribute"
      ],
      "metadata": {
        "id": "gFqWe0HUZIbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions pre-processing data\n",
        "\n",
        "# drop duplicates before processing\n",
        "dataset_distinct = df.drop_duplicates()\n",
        "# we now drop the ids column (since we are sure duplicates have been removed)\n",
        "dataset = dataset_distinct.drop('ID', axis=1)\n",
        "\n",
        "def replace_missing_data(dataframe):\n",
        "  grouped = dataframe.groupby(\"Diagnosis\")\n",
        "\n",
        "  for attribute in dataframe.columns[1:len(dataframe.columns)] :\n",
        "    mean_values = grouped[attribute].transform('mean')\n",
        "    dataframe[attribute].fillna(mean_values, inplace=True)\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "dataset_post_process = replace_missing_data(dataset)\n",
        "\n",
        "dataset_post_process.head(10)\n"
      ],
      "metadata": {
        "id": "S-U9Szoj9-Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now visualize our data in order to better understand the attributes being used."
      ],
      "metadata": {
        "id": "pQPmXk2gQHKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 10\n",
        "num_cols = 3\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(30, 50))\n",
        "fig.suptitle(\"Box and Whisker Plots for Wisconsin Breast Cancer Dataset\", fontsize=16)\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "\n",
        "for i, attribute in enumerate(dataset_labels[2:len(dataset_labels)]):\n",
        "    ax = axes[i]\n",
        "    boxplot = ax.boxplot(dataset_post_process[attribute], vert=False)\n",
        "    ax.set_title(attribute)\n",
        "\n",
        "    # Calculate statistics\n",
        "    quartiles = np.percentile(dataset_post_process[attribute], [25, 50, 75])\n",
        "    q1 = quartiles[0]\n",
        "    median = quartiles[1]\n",
        "    q3 = quartiles [2]\n",
        "    whisker_min = boxplot['whiskers'][0].get_xdata()[1]  # Minimum whisker\n",
        "    whisker_max = boxplot['whiskers'][1].get_xdata()[1]  # Maximum whisker\n",
        "\n",
        "    ax.text(0.05, 0.85, f\"Q1: {quartiles[0]:.4f}\", transform=ax.transAxes)\n",
        "    ax.text(0.05, 0.75, f\"Median: {quartiles[1]:.4f}\", transform=ax.transAxes)\n",
        "    ax.text(0.05, 0.65, f\"Q3: {quartiles[2]:.4f}\", transform=ax.transAxes)\n",
        "    ax.text(0.5, 0.85, f\"Min: {whisker_min:.4f}\", transform=ax.transAxes)\n",
        "    ax.text(0.5, 0.75, f\"Max: {whisker_max:.4f}\", transform=ax.transAxes)\n",
        "\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lhOR1f54HzNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe from the figures above that there are no outliers below the min of any attributes. Infact all outliers lie above the max value.\n",
        "\n",
        "We can also observe that for most attributes, the data is skewed to the right.\n",
        "This is an aspect that can affect the predictions of the model and possibly introduce bias.\n"
      ],
      "metadata": {
        "id": "3e7Ht1dzOkiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now normalize the attributes and binarize our Diagnosis column in preparation for feeding the data to the KNN model."
      ],
      "metadata": {
        "id": "8VopoMBBQRpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize_data(dataframe):\n",
        "  # we scale all attributes between 0 and 1\n",
        "  # we binarize class/diagnosis\n",
        "\n",
        "  diagnosis = dataframe[\"Diagnosis\"]\n",
        "  features = dataframe.drop(\"Diagnosis\",axis = 1) # inplace maintains order of rows after dropping column\n",
        "  diagnosis.replace({'M': 1,'B':0},inplace = True) # inplace maintains order of rows after replacing\n",
        "\n",
        "  x = features.values\n",
        "  min_max_scaler = MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_normalized = pd.DataFrame(x_scaled,columns = features.columns)\n",
        "  df_normalized[\"Diagnosis\"] = diagnosis\n",
        "\n",
        "  return df_normalized\n",
        "\n",
        "df_normalized = normalize_data(dataset_post_process)\n",
        "\n",
        "df_normalized.head(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "2XYoJf-jGW1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f1 score to score performance\n",
        "\n",
        "y = df_normalized[\"Diagnosis\"]\n",
        "X = df_normalized.drop(\"Diagnosis\",axis = 1)\n",
        "\n",
        "# split using standard 80% training and 20% test. We stratify our data in order to maintain class frequency\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, stratify = y,random_state=42)\n",
        "\n",
        "k_values_scores = {}\n",
        "\n",
        "for k in range(1,11):\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  y_pred = knn.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  k_values_scores[k] = f1\n",
        "\n",
        "best_K = max(k_values_scores, key=k_values_scores.get)\n",
        "\n",
        "print(f\"The best K value, with the highest F1 score is {best_K} with an F1 score of {k_values_scores[best_K]}.\")\n",
        "\n",
        "k,score = zip(*k_values_scores.items())\n",
        "\n",
        "sn.lineplot(x = k, y = score, marker = 'o')\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"F1 Score\")\n"
      ],
      "metadata": {
        "id": "8yFQNDQZIyuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that given current train test split ratio and random state of 42, the K value of 5 gives the highest F1 score of 0.95.\n",
        "\n",
        "This can also be observes in the above line graph.\n",
        "\n",
        "We can also observer that the F1 score remains relatively unchanged from K = 6 to K = 10.\n",
        "\n",
        "There is a large jump from K = 1 to 3 and then a slight decrease at K = 4.\n",
        "\n",
        "We must however note that many attributes in our dataset were skewed. This will likely introduce bias to our model. In order to correct this, more data must be collected, ensuring that it is not biased."
      ],
      "metadata": {
        "id": "1GQM_x7N2vWv"
      }
    }
  ]
}